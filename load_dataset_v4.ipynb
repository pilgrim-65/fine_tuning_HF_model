{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45921d14-ea20-41ec-8acb-fb3ecaf4c34e",
   "metadata": {},
   "source": [
    "# IberLEF 2023 Task - PoliticEs. Political ideology detection in Spanish texts\n",
    "\n",
    "Datasources:\n",
    "\n",
    "[https://portal.odesia.uned.es/en/dataset/politices-2023](https://portal.odesia.uned.es/en/dataset/politices-2023)\n",
    "\n",
    "[https://codalab.lisn.upsaclay.fr/competitions/10173#learn_the_details-get_starting_kit](https://codalab.lisn.upsaclay.fr/competitions/10173#learn_the_details-get_starting_kit)\n",
    "\n",
    "Using Trainer class with CUDA parameters.\n",
    "\n",
    "RoBERTuito is a superior model for this task because it was specifically pre-trained on 500 million Spanish tweets, making it highly attuned to the informal language, slang, and structures found in the PoliticES dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8a4bc",
   "metadata": {},
   "source": [
    "If do we need to check torch and cuda versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0fef0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version.cuda='12.8'\n",
      "torch.__version__='2.9.0+cu128'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"{torch.version.cuda=}\\n{torch.__version__=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152764e4",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f01510-3b7c-4125-bd7e-18478a3761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1343ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and shuffled. Total rows: 180000\n",
      "\n",
      "--- Final Dataset Split Sizes ---\n",
      "Train size:      144000\n",
      "Validation size: 18000\n",
      "Test size:       18000\n",
      "Total rows:      180000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_PATH = 'data/politicES_phase_2_train_public.csv'\n",
    "RANDOM_SEED = 42 # Use a fixed seed for reproducibility\n",
    "\n",
    "# Define the splitting ratios (must sum to 1.0)\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1 \n",
    "\n",
    "# --- 1. Load Data and Initial Shuffle ---\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# 1.a) Shuffle the DataFrame rows\n",
    "# The 'sample' method with frac=1.0 shuffles all rows.\n",
    "# Use the same random state for consistent shuffling.\n",
    "df_shuffled = df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "print(f\"Data loaded and shuffled. Total rows: {len(df_shuffled)}\")\n",
    "\n",
    "\n",
    "# 2. Convert to Hugging Face Dataset\n",
    "raw_dataset = Dataset.from_pandas(df_shuffled)\n",
    "\n",
    "\n",
    "# 3. Create Train, Validation, and Test Splits\n",
    "\n",
    "# 3.a) Split the initial dataset into a temporary training set (80%) and a holdout set (20%)\n",
    "# The datasets library uses 'train' and 'test' keys for the split\n",
    "# The size of the test split here is 1.0 - TRAIN_RATIO (which is 0.2)\n",
    "train_test_split = raw_dataset.train_test_split(\n",
    "    test_size=(VAL_RATIO + TEST_RATIO), # 0.1 + 0.1 = 0.2\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Rename the splits for clarity\n",
    "train_ds = train_test_split['train']\n",
    "holdout_ds = train_test_split['test']\n",
    "\n",
    "# 3.b) Split the holdout set (20%) into validation (10%) and test (10%)\n",
    "# We need to calculate the ratio relative to the holdout set (0.1 / 0.2 = 0.5)\n",
    "val_test_split = holdout_ds.train_test_split(\n",
    "    test_size=VAL_RATIO / (VAL_RATIO + TEST_RATIO), # 0.1 / 0.2 = 0.5\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 4. Combine into the final DatasetDict\n",
    "politic_dataset = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': val_test_split['train'], # The 'train' part of this split is the validation set\n",
    "    'test': val_test_split['test']        # The 'test' part of this split is the final test set\n",
    "})\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Final Dataset Split Sizes ---\")\n",
    "print(f\"Train size:      {len(politic_dataset['train'])}\")\n",
    "print(f\"Validation size: {len(politic_dataset['validation'])}\")\n",
    "print(f\"Test size:       {len(politic_dataset['test'])}\")\n",
    "print(f\"Total rows:      {len(politic_dataset['train']) + len(politic_dataset['validation']) + len(politic_dataset['test'])}\")\n",
    "\n",
    "\n",
    "# The 'politic_dataset' object is ready to be used with your tokenizer and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88f4b0d-98f5-4bbc-994f-91f979915d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'gender', 'profession', 'ideology_binary', 'ideology_multiclass', 'tweet'],\n",
       "        num_rows: 144000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'gender', 'profession', 'ideology_binary', 'ideology_multiclass', 'tweet'],\n",
       "        num_rows: 18000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'gender', 'profession', 'ideology_binary', 'ideology_multiclass', 'tweet'],\n",
       "        num_rows: 18000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b82ddc-eacb-48b3-9dd8-9437f37d7a65",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2e350a8-8678-4402-a6cc-103894878639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "# Define the new RoBERTuito checkpoint\n",
    "checkpoint = \"pysentimiento/robertuito-base-cased\"\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "\n",
    "# Load Tokenizer and Model\n",
    "# Note: The original authors recommend using the cased version.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.model_max_length = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e82397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlozanojm65\u001b[0m (\u001b[33mlozanojm65-home\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jmlv/learning/transformers/unit04/wandb/run-20251114_113328-429lntjg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lozanojm65-home/transformers-fine-tuning/runs/429lntjg' target=\"_blank\">politicES-robertuito-base-cased-large-dataset</a></strong> to <a href='https://wandb.ai/lozanojm65-home/transformers-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lozanojm65-home/transformers-fine-tuning' target=\"_blank\">https://wandb.ai/lozanojm65-home/transformers-fine-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lozanojm65-home/transformers-fine-tuning/runs/429lntjg' target=\"_blank\">https://wandb.ai/lozanojm65-home/transformers-fine-tuning/runs/429lntjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lozanojm65-home/transformers-fine-tuning/runs/429lntjg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fabdc5a8c20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "load_dotenv(\".env\")  # Load environment variables WANDB_API_KEY \n",
    "wandb.init(project=\"transformers-fine-tuning\", name=f\"politicES-{model_name}-large-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df438813-48b6-4014-ad0b-84025bc205f9",
   "metadata": {},
   "source": [
    "## 3. Define the Classification Task and Labels\n",
    "The target column is **'ideology_binary'**. We need to map the labels to IDs,\n",
    "where 'ideology_binary' has two classes: 'left' and 'right'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6670883-331d-4dc2-8fa5-ece8f4babba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings \n",
    "label_to_id = {\"left\": 0, \"right\": 1}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "num_labels = len(label_to_id)\n",
    "\n",
    "# Load the model with the correct number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd45d5af",
   "metadata": {},
   "source": [
    "\n",
    "The **'label'** column is usually the one used by the Trainer, \n",
    "so we will map the target column **'ideology_binary'** to **'label'**. In the PoliticES dataset the **'label'** column corresponds to an id of the tweet, so we have to rename it. \n",
    "\n",
    "Moreover, the parameter **remove_unused_columns** of the Trainer class defaults to True, we don't need to take care of removing the columns unused by the model forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c134c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = politic_dataset.remove_columns(['label', 'gender', 'profession',  'ideology_multiclass'])\n",
    "politic_dataset = politic_dataset.rename_column(\"label\", \"id\")\n",
    "politic_dataset = politic_dataset.rename_column(\"ideology_binary\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97029137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'gender', 'profession', 'label', 'ideology_multiclass', 'tweet'],\n",
       "        num_rows: 144000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'gender', 'profession', 'label', 'ideology_multiclass', 'tweet'],\n",
       "        num_rows: 18000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'gender', 'profession', 'label', 'ideology_multiclass', 'tweet'],\n",
       "        num_rows: 18000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a94b96",
   "metadata": {},
   "source": [
    "To ensure optimal performance, we must apply the preprocess_tweet function from the pysentimiento library before tokenizing the text. This preprocessor is specially suited for tweet classification with transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f698d61-d8cf-4d1f-92d1-25daf3151310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b02690837d4338b868ce4e2faba76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480e3ca5ed1842bc81432f81da08fb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da48462ddee48fb874367a3aaac7c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def tokenize_and_encode_labels(examples):\n",
    "    # Step A: Apply tweet-specific cleaning/normalization\n",
    "    preprocessed_tweets = [preprocess_tweet(text) for text in examples[\"tweet\"]]\n",
    "    \n",
    "    # Step B: Tokenize the preprocessed text\n",
    "    tokenized_inputs = tokenizer(\n",
    "        preprocessed_tweets, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Step C: Map the political ideology string label to an integer ID\n",
    "    tokenized_inputs[\"label\"] = [label_to_id[label] for label in examples[\"label\"]]\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = politic_dataset.map(tokenize_and_encode_labels, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca44da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'gender', 'profession', 'label', 'ideology_multiclass', 'tweet', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 144000\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0867756a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value('string'),\n",
       " 'gender': Value('string'),\n",
       " 'profession': Value('string'),\n",
       " 'label': Value('int64'),\n",
       " 'ideology_multiclass': Value('string'),\n",
       " 'tweet': Value('string'),\n",
       " 'input_ids': List(Value('int32')),\n",
       " 'token_type_ids': List(Value('int8')),\n",
       " 'attention_mask': List(Value('int8'))}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01cf3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018672a",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Metrics\n",
    "\n",
    "* Precission measures how many of the samples predicted as positive are actually positive.\n",
    "* Recall measures how many of the positive samples by the positive predictions.\n",
    "* And one way to summarize both metrics is the f1-score, which is the armonic mean of precission and recall:\n",
    "<center>\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "$F1 = 2 \\times \\large \\frac{precision\\times recall}{precision + recall}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb934aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load the necessary metrics from the 'evaluate' library\n",
    "# We can load the standard 'precision', 'recall', and 'f1' metrics once\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 macro score for a Hugging Face Trainer.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (EvalPrediction): A tuple (predictions, labels) provided by Trainer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'precision', 'recall', and 'f1-macro' metrics.\n",
    "    \"\"\"\n",
    "    # The EvalPrediction object contains (predictions, label_ids)\n",
    "    logits, labels = eval_pred \n",
    "\n",
    "    # 1. Convert logits to class predictions\n",
    "    # Predictions are the index of the highest logit value across the class axis (-1)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # 2. Compute the metrics using the macro average\n",
    "    \n",
    "    f1_result = metric_f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    precision_result = metric_precision.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall_result = metric_recall.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    \n",
    "    # 3. Return the results dictionary\n",
    "    return {\n",
    "        \"precision\": precision_result[\"precision\"],\n",
    "        \"recall\": recall_result[\"recall\"],\n",
    "        \"f1-macro\": f1_result[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccaece",
   "metadata": {},
   "source": [
    "## 5. Configure Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce51ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "output_dir = f\"./results_{model_name}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,                # Output directory\n",
    "    num_train_epochs=5,                   # Total number of training epochs\n",
    "    per_device_train_batch_size=32,       # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,        # Batch size for evaluation\n",
    "    fp16=True,                            # Use mixed precision\n",
    "    eval_strategy=\"epoch\",                # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,          # Load the best model found during training\n",
    "    metric_for_best_model=\"eval_loss\",    # Metric to track for best model\n",
    "    disable_tqdm=False,                   # Enable tqdm progress bars\n",
    "    report_to=\"wandb\"                     # Report metrics to Weights & Biases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "676770ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95883f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18000' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18000/22500 49:46 < 12:26, 6.03 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.492431</td>\n",
       "      <td>0.745295</td>\n",
       "      <td>0.743844</td>\n",
       "      <td>0.744483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389400</td>\n",
       "      <td>0.505684</td>\n",
       "      <td>0.760346</td>\n",
       "      <td>0.759590</td>\n",
       "      <td>0.759945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.602437</td>\n",
       "      <td>0.759324</td>\n",
       "      <td>0.757800</td>\n",
       "      <td>0.758474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.142700</td>\n",
       "      <td>0.820451</td>\n",
       "      <td>0.755374</td>\n",
       "      <td>0.750150</td>\n",
       "      <td>0.751960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18000, training_loss=0.3327179616292318, metrics={'train_runtime': 2986.4978, 'train_samples_per_second': 241.085, 'train_steps_per_second': 7.534, 'total_flos': 2.519653421661312e+16, 'train_loss': 0.3327179616292318, 'epoch': 4.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final evaluation on the dedicated test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [563/563 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Set Evaluation Results ---\n",
      "eval_loss: 0.4955\n",
      "eval_precision: 0.7464\n",
      "eval_recall: 0.7448\n",
      "eval_f1-macro: 0.7454\n",
      "eval_runtime: 19.1011\n",
      "eval_samples_per_second: 942.3520\n",
      "eval_steps_per_second: 29.4750\n",
      "epoch: 4.0000\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'final_datasets' is your DatasetDict with 'train', 'validation', and 'test' keys\n",
    "# And 'trainer' is your instantiated Hugging Face Trainer\n",
    "\n",
    "# 1. Select the dedicated test split\n",
    "test_dataset = tokenized_dataset['test']\n",
    "\n",
    "print(\"Starting final evaluation on the dedicated test set...\")\n",
    "\n",
    "# 2. Call the evaluate method\n",
    "# The Trainer will automatically use the GPU (if available) and batch processing\n",
    "evaluation_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "print(\"\\n--- Final Test Set Evaluation Results ---\")\n",
    "# 3. Print the results\n",
    "# The output will include your custom metrics ('precision', 'recall', 'f1-macro') \n",
    "# along with standard Trainer metrics ('eval_loss', 'eval_runtime', etc.)\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637dcec",
   "metadata": {},
   "source": [
    "And finally, we can clear the used memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20711bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# del trainer\n",
    "# del model\n",
    "# del tokenized_dataset\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
